{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook intends to train an RNN to produce czech names resembling those that are used for naming biological spicies. I wrote this notebook in a colaboration with my friend Adam Blazek who did the data gathering.\n",
    "\n",
    "Text generation is based on the following article: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "Dominik Farhan, April 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we muse getter the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python version: 3.7.7\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "from platform import python_version\n",
    "print(f'Running Python version: {python_version()}')\n",
    "\n",
    "import os\n",
    "import re\n",
    "from sys import stderr\n",
    "from typing import Iterator\n",
    "from urllib.request import urlopen\n",
    "from random import choice, randrange\n",
    "from itertools import islice\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "def names_list_to_ids(names:list, char_to_id:dict) -> list:  \n",
    "    return [name_to_ids(n, char_to_id) for n in names]\n",
    "\n",
    "def name_to_ids(name:str, char_to_id:dict) -> list: \n",
    "    return [char_to_id[c] for c in name]\n",
    "\n",
    "def ids_to_name(char_inds: list, id_to_char:dict) -> str: \n",
    "    print(char_inds)\n",
    "    return ''.join([id_to_char[i] for i in char_inds])\n",
    "\n",
    "def ids_list_to_names(inds_names:list, id_to_char:dict) -> list: \n",
    "    return [ids_to_name(ids, id_to_char)for ids in inds_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scrapping\n",
    "\n",
    "def wikipedia_table_names(url: str, additional_names=True) -> Iterator[str]:\n",
    "    def process_name(rawname: str) -> Iterator[str]:\n",
    "        rawname = re.sub(r\"\\s+\", \" \", rawname).strip()\n",
    "        if re.search(r\"[_\\d]\", rawname):\n",
    "            return\n",
    "        match = re.match(r\"(\\w[\\w\\s.]*\\w\\s)(\\w+/[\\w/]+)\", rawname)\n",
    "        if match:\n",
    "            for variant in re.split(r\"/\", match[2]):\n",
    "                yield (match[1] + variant).lower()\n",
    "            return\n",
    "        match = re.match(r\"(\\w+/[\\w/]+)(\\s\\w[\\w\\s.]*\\w)\", rawname)\n",
    "        if match:\n",
    "            for variant in re.split(r\"/\", match[1]):\n",
    "                yield (variant + match[2]).lower()\n",
    "            return\n",
    "        for name in re.split(r\"\\s*/\\s*\", rawname):\n",
    "            yield name.lower()\n",
    "\n",
    "    page = urlopen(url).read().decode(\"utf-8\")\n",
    "    for row in re.findall(r\"<tr>\\n.*\", page):\n",
    "        if re.search(r\"Český název\", row):\n",
    "            continue\n",
    "        row = re.sub(r\"<.*?>\", \"\", row).strip()\n",
    "        match = re.search(r\"(\\w[\\w\\s./]*\\w)(?:\\s\\(([^)<]*))?\", row)\n",
    "        if match:\n",
    "            for name in process_name(match[1]):\n",
    "                yield name\n",
    "            if additional_names and match[2]:\n",
    "                for rawname in re.findall(r\"\\w[\\w\\s./]*\\w\", match[2]):\n",
    "                    for name in process_name(rawname):\n",
    "                        yield name\n",
    "        else:\n",
    "            print(f\"Couldn't match: '{row}'\", file=stderr)\n",
    "            \n",
    "def botany_names(url: str) -> Iterator[str]:\n",
    "    page = urlopen(url).read().decode(\"utf-8\")\n",
    "    for name in re.findall(r\"–\\s+([\\w\\s.]*)\", page):\n",
    "        if name:\n",
    "            yield name.lower()\n",
    "            \n",
    "\n",
    "sources = [\n",
    "    (\n",
    "        wikipedia_table_names,\n",
    "        \"https://cs.wikipedia.org/wiki/Seznam_l%C3%A9%C4%8Div%C3%BDch_rostlin\",\n",
    "    ),\n",
    "    (\n",
    "        wikipedia_table_names,\n",
    "        \"https://cs.wikipedia.org/wiki/Seznam_nejjedovat%C4%9Bj%C5%A1%C3%ADch_rostlin\",\n",
    "    ),\n",
    "    (botany_names, \"https://botany.cz/cs/kvetena-ceske-republiky/\"),\n",
    "]\n",
    "\n",
    "            \n",
    "def lnames(sources: list, save_to_file = False) -> list:\n",
    "    \"\"\" Returns a list with names used for training. \"\"\"\n",
    "    names = []\n",
    "    for function, url in sources:\n",
    "        for name in function(url):\n",
    "            names.append(name)\n",
    "    names = sorted(set(names))\n",
    "    if save_to_file:\n",
    "        with open(\"czech_plant_names.txt\", \"w\") as output:\n",
    "            for name in names:\n",
    "                print(name, file=output)\n",
    "    return names\n",
    "\n",
    "def lwords(names: list) -> list: return [word for words in names for word in words.split()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = lnames(sources, save_to_file = True)\n",
    "names.append('rys ostrovid') # Just to get it to 'nicer' number, 3999 -> 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct names 4000\n",
      "Number of distinct characters among all words 46\n"
     ]
    }
   ],
   "source": [
    "vocab = set(c for w in names for c in w)\n",
    "print(f'Number of distinct names {len(names)}')\n",
    "print(f'Number of distinct characters among all words {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to prepare the data so that we can feed it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest name has 53 characters\n",
      "The shortest name has 3 characters\n"
     ]
    }
   ],
   "source": [
    "vocab.add('&') # & will be the ending char so model can output 'end'.\n",
    "\n",
    "char_to_id = {c:i for i, c in enumerate(sorted(vocab))}\n",
    "id_to_char = {v:k for k, v in char_to_id.items()}\n",
    "print(f'The longest name has {max([len(p) for p in names])} characters')\n",
    "print(f'The shortest name has {min([len(p) for p in names])} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First name numerised: [4, 5, 20, 4, 11, 29, 16, 45, 24, 1, 21, 22, 20, 18, 16]\n",
      "[4, 5, 20, 4, 11, 29, 16, 45, 24, 1, 21, 22, 20, 18, 16]\n",
      "First name converted back: abrahámův strom\n"
     ]
    }
   ],
   "source": [
    "# Convert chars in names to indicies\n",
    "numerized = names_list_to_ids(names, char_to_id)\n",
    "print(f'First name numerised: {numerized[0]}')\n",
    "print(f'First name converted back: {ids_to_name(numerized[0], id_to_char)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (20, None, 256)           12032     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (20, None, 64)            61824     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (20, None, 47)            3055      \n",
      "=================================================================\n",
      "Total params: 76,911\n",
      "Trainable params: 76,911\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"last_checkpoint\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits): \n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    #print(input_text)\n",
    "    #print(target_text)\n",
    "    return input_text, target_text\n",
    "\n",
    "def generate_training_data(names, max_len):\n",
    "    while True:\n",
    "        for name in names:\n",
    "            x,y = split_input_target(name)\n",
    "            x += [char_to_id['&']] * (max_len - 1 - len(x)) \n",
    "            y += [char_to_id['&']] * (max_len - 1 - len(y)) \n",
    "            yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "gen = generate_training_data(numerized, 54)\n",
    "for i in range(len(numerized)):\n",
    "    x,y = next(gen)\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 53)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 12/50\n",
      "Epoch 13/50\n",
      "Epoch 14/50\n",
      "Epoch 15/50\n",
      "Epoch 16/50\n",
      "Epoch 17/50\n",
      "Epoch 18/50\n",
      "Epoch 19/50\n",
      "Epoch 20/50\n",
      "Epoch 21/50\n",
      "Epoch 22/50\n",
      "Epoch 23/50\n",
      "Epoch 24/50\n",
      "Epoch 25/50\n",
      "Epoch 26/50\n",
      "Epoch 27/50\n",
      "Epoch 28/50\n",
      "Epoch 29/50\n",
      "Epoch 30/50\n",
      "Epoch 31/50\n",
      "Epoch 32/50\n",
      "Epoch 33/50\n",
      "Epoch 34/50\n",
      "Epoch 35/50\n",
      "Epoch 36/50\n",
      "Epoch 37/50\n",
      "Epoch 38/50\n",
      "Epoch 39/50\n",
      "Epoch 40/50\n",
      "Epoch 41/50\n",
      "Epoch 42/50\n",
      "Epoch 43/50\n",
      "Epoch 44/50\n",
      "Epoch 45/50\n",
      "Epoch 46/50\n",
      "Epoch 47/50\n",
      "Epoch 48/50\n",
      "Epoch 49/50\n",
      "Epoch 50/50\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=EPOCHS, verbose = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model/last_epoch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            12032     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 64)             61824     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 47)             3055      \n",
      "=================================================================\n",
      "Total params: 76,911\n",
      "Trainable params: 76,911\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights('model/last_epoch_model')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_generator:\n",
    "    \"\"\" \n",
    "        A class for generating biological names.\n",
    "        \n",
    "        params:\n",
    "            model: model in keras trained to predict text.\n",
    "            names: a list of biological names (usually two words per string but not needed)\n",
    "            temperature: optional(0.2), it 'provides' a randomness to the prediction.\n",
    "                        The higher the value the more weird and unpredictable result look.\n",
    "                        The smaller the temprature the more will the value resemble a real biological name.\n",
    "                               \n",
    "    \"\"\"\n",
    "    def __init__(self, model,names, temperature = 0.2):\n",
    "        self.model = model\n",
    "        self.names = names\n",
    "        self.words = lwords(names)\n",
    "        self.temperature = temperature\n",
    "        self.part_gen = self.generate_part_of_word()\n",
    "        \n",
    "    def generate_part_of_word(self):\n",
    "        while True:\n",
    "            w = choice(self.words)\n",
    "            if self.ok_ending(w) and len(w) > 1:\n",
    "                yield w[:randrange(1,len(w))]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self) -> str:\n",
    "        part = next(self.part_gen)\n",
    "        t = time()\n",
    "        while True:\n",
    "            pred = self.generate_text(part, num_generate = 54)\n",
    "            name = self.get_name(pred)\n",
    "            if self.ok_ending(name.split()[0]):\n",
    "                break\n",
    "            if time() - t > 0.3: \n",
    "                # Net can get into a cycle in that case different starting part is needed\n",
    "                part = next(self.part_gen)\n",
    "                t = time()\n",
    "        return name\n",
    "        \n",
    "    def generate_text(self, start_string: str, num_generate = 60) -> str:\n",
    "        \"\"\" Generates a name string of a given lenght. \"\"\"\n",
    "\n",
    "        input_eval = [char_to_id[s] for s in start_string]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "        text_generated = []\n",
    "\n",
    "\n",
    "        self.model.reset_states()\n",
    "        for i in range(num_generate):\n",
    "            predictions = self.model(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            predictions = predictions / self.temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "            text_generated.append(id_to_char[predicted_id])\n",
    "\n",
    "        return (start_string + ''.join(text_generated))\n",
    "\n",
    "\n",
    "    def get_name(self, NN_pred: str) -> str: return ' '.join(NN_pred.split()[:2])\n",
    "\n",
    "    def get_start(self) -> str: return choice(self.words)\n",
    "    \n",
    "    def ok_ending(self, word): return not word[-1] in 'ýáíéů'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Text_generator(model, names, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kozice setá\n",
      "turne vonný\n",
      "rapontikavice chlubatý\n",
      "růžka poutinaláské\n",
      "chrpa hlavatý\n",
      "menka šilicovitý\n",
      "hluchav setý\n",
      "tět střídovatý\n",
      "kalka. polní\n",
      "jeten kozí\n",
      "jestřábník obecný\n",
      "baďyně tuhátolistý\n",
      "žbábník vonný\n",
      "cinkatec maloplodý\n",
      "mandlík krasanový\n",
      "lavsoniet setý\n",
      "kvasička krasnicovitý\n",
      "béřka parovatý\n",
      "kavič stromovitý\n",
      "kamzičník kamerinovitý\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5473289489746094"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "for name in islice(gen,0,20):\n",
    "    print(name)\n",
    "(time() - t)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afrástý tamatinatý\n",
      "běžluk setý\n",
      "chýl setý\n",
      "dyněličník jadlý\n",
      "elček setý\n",
      "fiaalka. posnědý\n",
      "gajník prososný\n",
      "hýl klaný\n",
      "ivá bahenník\n",
      "javaný klasnatý\n",
      "kardský tenolistá\n",
      "listý balpský\n",
      "menná setá\n",
      "nátkový stromovitý\n",
      "ový stříbníkovitý\n",
      "pý kapská\n",
      "rý vonný\n",
      "ská zahradníkovitý\n",
      "tolistý pazpovníkovátec\n",
      "ubířský prodloditý\n",
      "vanový chlupatý\n",
      "wasovný kapská\n",
      "xajas červené\n",
      "zdník evořský\n",
      "y rozpový\n"
     ]
    }
   ],
   "source": [
    "# What if all the words start with a letter from alphabet? ('q' in unknown to the model)\n",
    "for letter in 'abcdefghijklmnoprstuvwxzy':\n",
    "    print(' '.join(gen.generate_text(letter, num_generate = 100).split()[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z\n",
      "v\n"
     ]
    }
   ],
   "source": [
    "# This is interesting, there are 'words' that have lenght 1...\n",
    "for w in gen.words:\n",
    "    if len(w) == 1:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existuje snad přece jen v některých případech možnost\n",
      "\n",
      "panenka v trní\n",
      "\n",
      "růže z jericha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Those are all correct names.\n",
    "# However are model cant predict those because its bounded to precting two-word names\n",
    "for n in gen.names:\n",
    "    # This checks for some positions of z and v, but not all of them.\n",
    "    for c in [' v ', ' z ']:\n",
    "        if c in n:\n",
    "            print(n)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is all that is needed to use the generator outside this notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "from random import choice, randrange\n",
    "from time import time\n",
    "from itertools import islice\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bio_names_generator:\n",
    "    \"\"\" \n",
    "        A class for generating biological names.\n",
    "        \n",
    "        params:\n",
    "            model: optional (None), model in keras trained to predict text.\n",
    "            names: optional (None), a list of biological names (usually two words per \n",
    "                   string but not needed). If None 'load_names' is called\n",
    "            temperature: optional (0.2), it 'provides' a randomness to the prediction.\n",
    "                        The higher the value the more weird and unpredictable result look.\n",
    "                        The smaller the temprature the more will the value resemble a real \n",
    "                        biological name.\n",
    "                               \n",
    "    \"\"\"\n",
    "    def __init__(self, model = None,names = None, temperature = 0.2):\n",
    "        if names == None:\n",
    "            self.load_names()\n",
    "        else:\n",
    "            self.names = names\n",
    "        self.words = self.lwords(self.names)\n",
    "        self.create_vocab()\n",
    "        if model == None:\n",
    "            self.load_model()\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.part_gen = self.generate_part_of_word()\n",
    "        self.temperature = temperature\n",
    "        self.char_to_id = {c:i for i, c in enumerate(sorted(self.vocab))}\n",
    "        self.id_to_char = {v:k for k, v in self.char_to_id.items()}\n",
    "    \n",
    "    # Preparatory functions:\n",
    "    \n",
    "    def create_vocab(self):\n",
    "        self.vocab = set(c for w in self.names for c in w)\n",
    "        #self.vocab.add('&') # Ending char.\n",
    "    \n",
    "    def build_model(self, embedding_dim = 256, rnn_units = 64, batch_size = 1):\n",
    "        \"\"\" Builds the model given some parameters of it.\n",
    "        \n",
    "            params:\n",
    "                None of the params should be changed unless the model architecture is changed!\n",
    "                embedding_dim: optional (256)\n",
    "                rnn_unnits: optional (64)\n",
    "                batch_size: optional (1) size of the batch should be 1, because the generator \n",
    "                            is built to predict one name at a time. However changing it and\n",
    "                            rewriting some other parts may speed up the whole process.\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                      batch_input_shape=[batch_size, None]),\n",
    "            tf.keras.layers.GRU(rnn_units,\n",
    "                                return_sequences=True,\n",
    "                                stateful=True,\n",
    "                                recurrent_initializer='glorot_uniform'),\n",
    "            tf.keras.layers.Dense(vocab_size)\n",
    "            ])\n",
    "        return model    \n",
    "        \n",
    "    def load_model(self, path_to_model = 'model/last_epoch_model'):\n",
    "        \"\"\" Loads the model given path\"\"\"\n",
    "        model = self.build_model()\n",
    "        model.load_weights(path_to_model)\n",
    "        model.build(tf.TensorShape([1, None]))\n",
    "        self.model = model\n",
    "        \n",
    "    def lwords(self, names: list) -> list:\n",
    "        \"\"\" Creates list of all words in names. \"\"\"\n",
    "        return list(set([word for words in names for word in words.split()]))\n",
    "        \n",
    "    def load_names(self, path = 'czech_plant_names.txt'):\n",
    "        \"\"\" Loads names, expects one name per line. \"\"\"\n",
    "        with open(path, 'r') as data:\n",
    "            self.names = [name for name in data]\n",
    "            \n",
    "    # The generating part: \n",
    "    \n",
    "    def generate_part_of_word(self):\n",
    "        \"\"\" Generator, yields substrings of random len of words that are good by 'ok_ending'. \"\"\"\n",
    "        while True:\n",
    "            w = choice(self.words)\n",
    "            if self.ok_ending(w) and len(w) > 1:\n",
    "                yield w[:randrange(1,len(w))]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self) -> str:\n",
    "        part = next(self.part_gen)\n",
    "        t = time()\n",
    "        while True:\n",
    "            pred = self.generate_text(part, num_generate = 54)\n",
    "            name = self.get_name(pred)\n",
    "            if self.ok_ending(name.split()[0]):\n",
    "                break\n",
    "            if time() - t > 0.3: \n",
    "                # Net can get into a cycle in that case different starting part is needed\n",
    "                part = next(self.part_gen)\n",
    "                t = time()\n",
    "        return name\n",
    "        \n",
    "    def generate_text(self, start_string: str, num_generate = 60) -> str:\n",
    "        \"\"\" Generates a name string of a given lenght. \"\"\"\n",
    "\n",
    "        input_eval = [self.char_to_id[s] for s in start_string]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "        text_generated = []\n",
    "\n",
    "\n",
    "        self.model.reset_states()\n",
    "        for i in range(num_generate):\n",
    "            predictions = self.model(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            predictions = predictions / self.temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "            text_generated.append(self.id_to_char[predicted_id])\n",
    "\n",
    "        return (start_string + ''.join(text_generated))\n",
    "\n",
    "\n",
    "    def get_name(self, NN_pred: str) -> str: return ' '.join(NN_pred.split()[:2])\n",
    "    \n",
    "    def ok_ending(self, word): return not word[-1] in 'ýáíéů'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Bio_names_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koflíček polník\n",
      "kaštěk polník\n",
      "andělika polní\n",
      "kokrhen polníkový\n",
      "borůvka vonná\n",
      "maďovec vonný\n",
      "krva dvoudovitá\n",
      "zlatičník kostřední\n",
      "ouška polní\n",
      "tras. prostřední\n",
      "rozchodník polník\n",
      "dvouřadník pravý\n",
      "ka polník\n",
      "pepřovník polní\n",
      "dýl tuhýchostřední\n",
      "cypřišek polník\n",
      "blík obecný\n",
      "chvoštět střešní\n",
      "zdník poponský\n",
      "růžka polní\n"
     ]
    }
   ],
   "source": [
    "for name in islice(gen,0,20):\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
